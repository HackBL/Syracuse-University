{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Download\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139641164\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import time\n",
    "\n",
    "path = \"/Users/boli/Desktop/nlp/HW1/clothing_shoes_jewelry.txt\"\n",
    "\n",
    "rfile = open(path,\"r\") \n",
    "contents = rfile.read()\n",
    "rfile.close()\n",
    "print(len(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2786318\n",
      "<class 'list'>\n",
      "['reviewerID:A1KLRMWW2FWPL4', 'asin:0000031887', 'reviewerName:Amazon Customer \"cameramom\"', 'helpful:[0, 0]', \"reviewText:This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\", 'overall:5.0', 'summary:Great tutu-  not cheaply made', 'unixReviewTime:1297468800', 'reviewTime:02 12, 2011', '']\n"
     ]
    }
   ],
   "source": [
    "lines = contents.splitlines()\n",
    "print(len(lines))\n",
    "print(type(lines))\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278677\n",
      "[\"This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\", 'I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and the teacher thought it was adorable. I bought this to go with a light blue long sleeve leotard and was happy the colors matched up great. Price was very good too since some of these go for over $15.00 dollars.', 'What can I say... my daughters have it in orange, black, white and pink and I am thinking to buy for they the fuccia one. It is a very good way for exalt a dancer outfit: great colors, comfortable, looks great, easy to wear, durables and little girls love it. I think it is a great buy for costumer and play too.']\n"
     ]
    }
   ],
   "source": [
    "# Remove title \"reviewText:\"\n",
    "reviewText = [r[11:] for r in lines if \"reviewText\" in r ] # r[11:] is contents after 'reviewText:'\n",
    "print(len(reviewText))\n",
    "print(reviewText[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save extracted reviews into file \n",
    "saveFile = open('/Users/boli/Desktop/nlp/HW1/reviews.txt', 'w')\n",
    "\n",
    "for r in reviewText:\n",
    "    saveFile.write(r + '\\n')\n",
    "\n",
    "saveFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\t  228.00179386138916\n",
      "19218012\n",
      "['This', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.', 'It', 'does', \"n't\", 'look', 'cheap', 'at', 'all', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenizer      \n",
    "tokens = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for t in reviewText:\n",
    "    tokens += nltk.word_tokenize(t) \n",
    "    \n",
    "print('time:\\t ' , time.time()-start)\n",
    "\n",
    "print(len(tokens))\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\t  2.9568090438842773\n",
      "19218012\n",
      "<class 'list'>\n",
      "['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.', 'it', 'does', \"n't\", 'look', 'cheap', 'at', 'all', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lowercase \n",
    "start = time.time()\n",
    "lowercase = [w.lower() for w in tokens] \n",
    "\n",
    "print('time:\\t ' , time.time()-start)\n",
    "\n",
    "print(len(lowercase))\n",
    "print(type(lowercase))\n",
    "print(lowercase[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\t  1.952406883239746\n",
      "16339294\n",
      "<class 'list'>\n",
      "['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', 'it', 'does', 'look', 'cheap', 'at', 'all', 'i', 'so', 'glad']\n"
     ]
    }
   ],
   "source": [
    "# alphabets\n",
    "start = time.time()\n",
    "alphaWords = [w for w in lowercase if w.isalpha()]\n",
    "\n",
    "print('time:\\t ' , time.time()-start)\n",
    "\n",
    "print(len(alphaWords))\n",
    "print(type(alphaWords))\n",
    "print(alphaWords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\t  2621.689451932907\n",
      "16339294\n",
      "<class 'list'>\n",
      "['this', 'be', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', 'it', 'do', 'look', 'cheap', 'at', 'all', 'i', 'so', 'glad']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizedWords = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in alphaWords]\n",
    "\n",
    "print('time:\\t ' , time.time()-start)\n",
    "\n",
    "print(len(lemmatizedWords))\n",
    "print(type(lemmatizedWords))\n",
    "print(lemmatizedWords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7734920\n",
      "<class 'list'>\n",
      "['great', 'tutu', 'really', 'great', 'price', 'look', 'cheap', 'glad', 'look', 'amazon', 'found', 'affordable', 'tutu', 'make', 'poorly', 'bought', 'yr', 'old', 'daughter', 'dance']\n"
     ]
    }
   ],
   "source": [
    "# Stop word list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_words = [w for w in lemmatizedWords if not w in stop_words ]\n",
    "\n",
    "print(len(filtered_words))\n",
    "print(type(filtered_words))\n",
    "print(filtered_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59347\n",
      "<class 'list'>\n",
      "['great', 'tutu', 'really', 'price', 'look', 'cheap', 'glad', 'amazon', 'found', 'affordable', 'make', 'poorly', 'bought', 'yr', 'old', 'daughter', 'dance', 'class', 'wore', 'today']\n"
     ]
    }
   ],
   "source": [
    "# frequency distribution with FreqDist\n",
    "from nltk import FreqDist\n",
    "fdist = FreqDist(filtered_words)\n",
    "fdistkeys = list(fdist.keys())\n",
    "\n",
    "print(len(fdistkeys))\n",
    "print(type(fdistkeys))\n",
    "print(fdistkeys[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wear', 108396)\n",
      "('fit', 107945)\n",
      "('size', 102016)\n",
      "('like', 100675)\n",
      "('look', 99305)\n",
      "('shoe', 81969)\n",
      "('love', 80277)\n",
      "('great', 78821)\n",
      "('get', 78391)\n",
      "('well', 75696)\n",
      "('would', 67419)\n",
      "('one', 64271)\n",
      "('good', 60579)\n",
      "('comfortable', 57492)\n",
      "('make', 56123)\n",
      "('order', 55518)\n",
      "('color', 52403)\n",
      "('nice', 46256)\n",
      "('really', 46108)\n",
      "('go', 44047)\n",
      "('small', 43360)\n",
      "('little', 42765)\n",
      "('bought', 42611)\n",
      "('pair', 40637)\n",
      "('foot', 40171)\n",
      "('time', 38949)\n",
      "('price', 37417)\n",
      "('work', 35352)\n",
      "('use', 34693)\n",
      "('quality', 33405)\n",
      "('watch', 32766)\n",
      "('perfect', 31928)\n",
      "('large', 31598)\n",
      "('also', 30149)\n",
      "('big', 29996)\n",
      "('much', 29977)\n",
      "('buy', 29491)\n",
      "('feel', 29343)\n",
      "('want', 28681)\n",
      "('purchase', 28105)\n",
      "('need', 27715)\n",
      "('even', 26972)\n",
      "('day', 26716)\n",
      "('recommend', 26497)\n",
      "('bit', 26084)\n",
      "('shirt', 26003)\n",
      "('bra', 25905)\n",
      "('long', 25806)\n",
      "('say', 25233)\n",
      "('come', 24192)\n"
     ]
    }
   ],
   "source": [
    "# 50 words by frequency\n",
    "topkeys = fdist.most_common(50)\n",
    "for pair in topkeys:\n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def alpha_filter(w):\n",
    "    pattern = re.compile('^[a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('.', 'i'), 0.014311573954683762)\n",
      "(('.', 'the'), 0.004950928327029871)\n",
      "((',', 'but'), 0.004068214756031997)\n",
      "((',', 'and'), 0.0036093743723336212)\n",
      "(('.', 'it'), 0.003552500643666993)\n",
      "(('they', 'are'), 0.003226868627202439)\n",
      "(('.', 'they'), 0.003019927347323958)\n",
      "(('i', 'have'), 0.0027907673280670236)\n",
      "(('in', 'the'), 0.0027346741171771563)\n",
      "((',', 'i'), 0.0027245794205977185)\n",
      "(('of', 'the'), 0.0025272645266326194)\n",
      "(('it', 'is'), 0.0024971365404496573)\n",
      "(('it', \"'s\"), 0.0024960958500806433)\n",
      "(('and', 'i'), 0.002308303272992024)\n",
      "(('.', 'this'), 0.0021367454656600277)\n",
      "(('and', 'the'), 0.002118845591312983)\n",
      "(('&', '#'), 0.0020227898702529687)\n",
      "(('is', 'a'), 0.001964042898922115)\n",
      "(('i', \"'m\"), 0.001951450545457043)\n",
      "(('i', 'am'), 0.0019086781712905582)\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(lowercase)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "first = scored[0]\n",
    "for bscore in scored[:20]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('they', 'are'), 0.003226868627202439)\n",
      "(('i', 'have'), 0.0027907673280670236)\n",
      "(('in', 'the'), 0.0027346741171771563)\n",
      "(('of', 'the'), 0.0025272645266326194)\n",
      "(('it', 'is'), 0.0024971365404496573)\n",
      "(('and', 'i'), 0.002308303272992024)\n",
      "(('and', 'the'), 0.002118845591312983)\n",
      "(('is', 'a'), 0.001964042898922115)\n",
      "(('i', 'am'), 0.0019086781712905582)\n",
      "(('a', 'little'), 0.0017551763418609584)\n",
      "(('on', 'the'), 0.0017513778220140564)\n",
      "(('i', 'was'), 0.001632322843798828)\n",
      "(('i', 'love'), 0.0016165043501898116)\n",
      "(('for', 'a'), 0.0015732116308388193)\n",
      "(('this', 'is'), 0.0015457374050968436)\n",
      "(('for', 'the'), 0.0015121751406961344)\n",
      "(('but', 'i'), 0.0014681019035683817)\n",
      "(('and', 'it'), 0.0014594121389871126)\n",
      "(('i', 'bought'), 0.0014244449425882344)\n",
      "(('i', 'would'), 0.0014110720713464014)\n",
      "(('if', 'you'), 0.001374127563246396)\n",
      "(('and', 'they'), 0.0013623677620765353)\n",
      "(('for', 'my'), 0.0013621075894842817)\n",
      "(('so', 'i'), 0.0013191270772439938)\n",
      "(('to', 'be'), 0.001318190455911881)\n",
      "(('these', 'are'), 0.0012929016799448352)\n",
      "(('it', 'was'), 0.0012451339920070818)\n",
      "(('to', 'wear'), 0.001189821298893975)\n",
      "(('i', 'do'), 0.0011168168695076266)\n",
      "(('a', 'bit'), 0.0011031317911550894)\n",
      "(('i', 'ordered'), 0.001091632162577482)\n",
      "(('when', 'i'), 0.0010154015930471892)\n",
      "(('the', 'price'), 0.00101181121127409)\n",
      "(('that', 'i'), 0.0010028612741005678)\n",
      "(('with', 'the'), 0.0009853256413826779)\n",
      "(('to', 'the'), 0.0009824637428678888)\n",
      "(('have', 'a'), 0.0009248094964245001)\n",
      "(('a', 'size'), 0.0009185653542104147)\n",
      "(('i', 'had'), 0.0008976474777932286)\n",
      "(('but', 'it'), 0.0008945254066861859)\n",
      "(('i', 'like'), 0.0008935887853540731)\n",
      "(('i', 'wear'), 0.0008907268868392839)\n",
      "(('like', 'the'), 0.0008883853335090019)\n",
      "(('in', 'a'), 0.0008826095019609729)\n",
      "(('is', 'very'), 0.0008765214633022396)\n",
      "(('i', 'got'), 0.0008672593190180129)\n",
      "(('with', 'a'), 0.0008577370021415326)\n",
      "(('for', 'me'), 0.00080856438220561)\n",
      "(('i', 'will'), 0.000782026777795747)\n",
      "(('i', 'can'), 0.0007801015006130707)\n"
     ]
    }
   ],
   "source": [
    "# alpha bigrams\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored1 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored1[:50]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('well', 'made'), 0.0004659691127261238)\n",
      "(('would', 'recommend'), 0.00032719305201807554)\n",
      "(('good', 'quality'), 0.0003203765301010323)\n",
      "(('highly', 'recommend'), 0.0002809863996338435)\n",
      "(('really', 'like'), 0.00026345076691595363)\n",
      "(('fit', 'perfectly'), 0.000232334124882428)\n",
      "(('fit', 'well'), 0.00022728677659270897)\n",
      "(('look', 'like'), 0.00022275977348749703)\n",
      "(('looks', 'great'), 0.0002128211804634111)\n",
      "(('another', 'pair'), 0.00020043696507214168)\n",
      "(('look', 'great'), 0.0001960140310038312)\n",
      "(('looks', 'like'), 0.00018826088775467515)\n",
      "(('feel', 'like'), 0.00018227691813284329)\n",
      "(('year', 'old'), 0.00018175657294833617)\n",
      "(('great', 'price'), 0.00017504412006819436)\n",
      "(('even', 'though'), 0.00016978863370467246)\n",
      "(('fit', 'great'), 0.00016609418289467194)\n",
      "(('usually', 'wear'), 0.00016177531786326285)\n",
      "(('light', 'weight'), 0.00015324165683734613)\n",
      "(('one', 'size'), 0.00014902686084283847)\n",
      "(('normally', 'wear'), 0.00014845448113988066)\n",
      "(('long', 'time'), 0.00014616496232804934)\n",
      "(('would', 'buy'), 0.00014569665166199293)\n",
      "(('fits', 'well'), 0.000144239685145373)\n",
      "(('every', 'day'), 0.0001399208201139639)\n",
      "(('arch', 'support'), 0.00013882809522649896)\n",
      "(('size', 'larger'), 0.0001353417824903013)\n",
      "(('look', 'good'), 0.0001345612647135406)\n",
      "(('little', 'bit'), 0.00012940984738692013)\n",
      "(('first', 'time'), 0.00012805694990720165)\n",
      "(('half', 'size'), 0.00012639184531677887)\n",
      "(('fits', 'perfectly'), 0.00012623574176142674)\n",
      "(('really', 'nice'), 0.00012587150013227175)\n",
      "(('much', 'better'), 0.0001243104645787504)\n",
      "(('great', 'quality'), 0.00011895090917832708)\n",
      "(('looks', 'good'), 0.0001181703914015664)\n",
      "(('high', 'quality'), 0.00011796225332776356)\n",
      "(('perfect', 'fit'), 0.00011603697614508722)\n",
      "(('different', 'colors'), 0.00011343525022255164)\n",
      "(('long', 'enough'), 0.00011182218015057957)\n",
      "(('fits', 'great'), 0.00011052131718931177)\n",
      "(('would', 'definitely'), 0.00010901231615424114)\n",
      "(('flip', 'flops'), 0.00010823179837748046)\n",
      "(('definitely', 'recommend'), 0.00010110306934973294)\n",
      "(('second', 'pair'), 0.00010084289675747938)\n",
      "(('really', 'cute'), 9.959406831466231e-05)\n",
      "(('super', 'cute'), 9.652403172607031e-05)\n",
      "(('size', 'smaller'), 9.60557210600139e-05)\n",
      "(('make', 'sure'), 9.600368654156319e-05)\n",
      "(('good', 'price'), 9.569147943085892e-05)\n"
     ]
    }
   ],
   "source": [
    "# stopword bigrams  -> Bigrams Result\n",
    "finder.apply_word_filter(lambda w: w in stop_words)\n",
    "scored2 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored2[:50]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('badgley', 'mischka'), 21.87402767411015)\n",
      "(('salvatore', 'exte'), 21.87402767411015)\n",
      "(('spatestruck', 'lenders'), 21.87402767411015)\n",
      "(('tessuto', 'vela'), 21.87402767411015)\n",
      "(('krav', 'maga'), 21.610993268276356)\n",
      "(('pepto', 'bismol'), 21.610993268276353)\n",
      "(('herman', 'munster'), 21.388600846939905)\n",
      "(('hypo', 'allergenic'), 21.388600846939905)\n",
      "(('birko', 'flor'), 21.19595576899751)\n",
      "(('myia', 'passiello'), 21.19595576899751)\n",
      "(('norman', 'reedus'), 21.19595576899751)\n",
      "(('hola', 'gente'), 21.16620842560346)\n",
      "(('saudi', 'arabia'), 21.16620842560346)\n",
      "(('charlotte', 'russe'), 20.87402767411015)\n",
      "(('giorgio', 'brutini'), 20.874027674110145)\n",
      "(('grady', 'harp'), 20.73652415036021)\n",
      "(('sherpani', 'soleil'), 20.710528941827267)\n",
      "(('laurel', 'burch'), 20.584521056915165)\n",
      "(('fecha', 'indicada'), 20.514131729023767)\n",
      "(('caslynn', 'lizzie'), 20.51145759472544)\n",
      "(('carolyn', 'pollack'), 20.441068266834044)\n",
      "(('vince', 'camuto'), 20.44106826683404)\n",
      "(('buenas', 'tardes'), 20.38860084693991)\n",
      "(('muk', 'luks'), 20.36998516877256)\n",
      "(('liz', 'claiborne'), 20.321486651081372)\n",
      "(('juanita', 'wilson'), 20.28906517338899)\n",
      "(('hanky', 'panky'), 20.195955768997514)\n",
      "(('strawberry', 'shortcake'), 20.19595576899751)\n",
      "(('yak', 'trax'), 20.178033861000248)\n",
      "(('bon', 'bebe'), 20.173587955969055)\n",
      "(('audrey', 'hepburn'), 20.14759274743611)\n",
      "(('muay', 'thai'), 20.0260307675552)\n",
      "(('darth', 'vader'), 20.010089223686176)\n",
      "(('nether', 'regions'), 19.99955855619401)\n",
      "(('hallux', 'limitus'), 19.973563347661063)\n",
      "(('alt', 'alt'), 19.965001334157638)\n",
      "(('gloria', 'vanderbilt'), 19.94802825555393)\n",
      "(('pom', 'poms'), 19.948028255553925)\n",
      "(('aurora', 'borealis'), 19.87402767411015)\n",
      "(('tai', 'chi'), 19.87402767411015)\n",
      "(('puerto', 'rico'), 19.874027674110145)\n",
      "(('buzz', 'lightyear'), 19.80363834621875)\n",
      "(('ros', 'hommerson'), 19.803638346218747)\n",
      "(('viet', 'nam'), 19.791565513918172)\n",
      "(('tuckable', 'lengthprior'), 19.736524150360214)\n",
      "(('prima', 'donna'), 19.688161128798814)\n",
      "(('haute', 'couture'), 19.66346068817049)\n",
      "(('koh', 'koh'), 19.610993268276356)\n",
      "(('onitsuka', 'tigers'), 19.610993268276353)\n",
      "(('libby', 'sue'), 19.558525848382217)\n"
     ]
    }
   ],
   "source": [
    "# Mutual Info Scored (min freq 5)\n",
    "finder.apply_freq_filter(5)\n",
    "scored3 = finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored3[:50]:\n",
    "    print(bscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
