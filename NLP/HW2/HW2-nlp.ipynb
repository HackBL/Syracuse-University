{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139641164\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import time\n",
    "\n",
    "path = \"\"/Users/boli/Desktop/\"nlp/HW2/clothing_shoes_jewelry.txt\"\n",
    "\n",
    "rfile = open(path,\"r\") \n",
    "contents = rfile.read()\n",
    "rfile.close()\n",
    "print(len(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2786318\n",
      "['reviewerID:A1KLRMWW2FWPL4', 'asin:0000031887', 'reviewerName:Amazon Customer \"cameramom\"', 'helpful:[0, 0]', \"reviewText:This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\", 'overall:5.0', 'summary:Great tutu-  not cheaply made', 'unixReviewTime:1297468800', 'reviewTime:02 12, 2011', '']\n"
     ]
    }
   ],
   "source": [
    "lines = contents.splitlines()\n",
    "print(len(lines))\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278677\n",
      "[\"This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\", 'I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and the teacher thought it was adorable. I bought this to go with a light blue long sleeve leotard and was happy the colors matched up great. Price was very good too since some of these go for over $15.00 dollars.', 'What can I say... my daughters have it in orange, black, white and pink and I am thinking to buy for they the fuccia one. It is a very good way for exalt a dancer outfit: great colors, comfortable, looks great, easy to wear, durables and little girls love it. I think it is a great buy for costumer and play too.']\n"
     ]
    }
   ],
   "source": [
    "# Remove title \"reviewText:\"\n",
    "reviewText = [r[11:] for r in lines if \"reviewText\" in r ] # r[11:] is contents after 'reviewText:'\n",
    "print(len(reviewText))\n",
    "print(reviewText[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88129771\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++,I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and the teacher thought it was ado\n"
     ]
    }
   ],
   "source": [
    "str_join = \",\".join(reviewText)\n",
    "\n",
    "print(len(str_join))\n",
    "print(str_join[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907576\n",
      "['This is a great tutu and at a really great price.', \"It doesn't look cheap at all.\", \"I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly.\", 'A++,I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and the teacher thought it was adorable.', 'I bought this to go with a light blue long sleeve leotard and was happy the colors matched up great.', 'Price was very good too since some of these go for over $15.00 dollars.,What can I say... my daughters have it in orange, black, white and pink and I am thinking to buy for they the fuccia one.', 'It is a very good way for exalt a dancer outfit: great colors, comfortable, looks great, easy to wear, durables and little girls love it.', 'I think it is a great buy for costumer and play too.,We bought several tutus at once, and they are got high reviews.', 'Sturdy and seemingly well-made.', 'The girls have been wearing them regularly, including out to play, and the tutus have stood up well.', 'Fits the 3-yr old & the 5-yr old well.', 'Clearly plenty of room to grow.', 'Only con is that when the kids pull off the tutus, the waste band gets twisted, and an adult has to un-tangle.', 'But this is not difficult.,Thank you Halo Heaven great product for Little Girls.', \"My Great Grand Daughters Love these Tutu's.\", 'Will buy more from this seller.', 'Made well and cute on the girls.', \"Thanks for a great product.NEVER BUY FROM DRESS UP DREAMS........I will buy more as long as I don't buy from &#34;Dress Up Dreams&#34;  I never rec'd or order in FL.\", \"Only rec'd pink, the purple one was missing.\", 'Company is a rip off.']\n"
     ]
    }
   ],
   "source": [
    "# sentences tokenize\n",
    "textsplit = nltk.sent_tokenize(str_join)\n",
    "\n",
    "print(len(textsplit))\n",
    "print(textsplit[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Analyze Sentence Start: ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\t  142444.28042697906\n",
      "-----Question:\n",
      "3682\n",
      "[\"I would definitely try other languages after seeing just how easy it is to use this stuff.What didn't I like?For starters the headphones and microphone that came with it are NOT quality, they are not comfortable and my ears got really hot.\", 'I was slightly disappointed that the headset headphones did not play the audio, and instead the computer speakers played the audio, so why do I need a headset for just the microphone?', 'Are you just the slightest bit interested in what your current altitude may be?', 'But I had to laugh when I realized I could not enter my own combination numbers.....the maker actually gives you the combination you have to use on your lock and then you get to memorize THEIR numbers.....how stupid are we getting?', \"The watch face was a little smaller than I expected, but nothing wrong with that.,Love it.it's a bit edgy and bothering at first but soon its very comfortable.does it job.Considering buying?\", 'She says they are very comfortable.,I remember buying these exact same Chucks for 39.99 not too long ago what happened since then?', \"If you have a good physique, what can I say - you're going to look hot in this shirt.\", 'Also, converse are great shoes.,Do these shoes always run so big?', \"What could be more perfect?,I was looking for a bag that was small enough that I wouldn't have to carry on my shoulder or would be too heavy (I have a medical condition).\", 'I sent it back.,Where do I even begin with how impressed I am with this wallet?', \"I'd recommend it still though because it beats having your straps fall down.,For $7 you get 4 of these things - what more can you ask?\", 'How in the heck did this bra get so many good reviews?', \"That sounds awful, doesn't it?\", 'What is not to like about this night gown?', \"Is 'necessary accessory' an oxymoron?Thanks, Hanes.\", \"How many products can honestly boast that?Marcianne Waters,I went ahead and purchased it, thinking i'd be getting tanned color.\", 'THe band was more comfortable (perhaps slightly loose?)', \"The problem?I think my Babies were a little too Big ... a little too much for Poor Old &#34;Felina|Harlow-Unlined Demi-Bra.&#34;They, I, didn't feel comfortable, nor safe walking or playing around in it.\", 'Just becareful maybe order a cup size larger for this particular design?,This bra fits, looks sexy, feels good and was just an all round wonderful find and good purchase.', 'Lots of detailing not really shown in the photo.,So, why am I happy with this?', \"What's not to love?\", \"The upside is they can be washed in very hot water and dried and I'm not afraid of damaging or shrinking them.,I've worn this underwear for years and gave up finding more (should have thought of Amazon earlier) The fit is perfect for me, high enough they don't slide off my hips, low enough they don't slide up the cheeks of my fanny, leg cut large enough to not cut off the circulation in my legs, more?\", 'Why is this necessary?', 'Most of them do the trick, but how comfortable are you?', \"my bf loves seeing me in these because it shows off my body which is a turn on for him ;) ya so go order these, or don't, more for me :),What can I say about Jockey underwear?\", 'Should be clear as black and white, right?', 'Great support, looks great, and comfortable...what more is there to say?,Very good quality slip-on.', 'A forgotten brand?', 'It didnt look as good as this one but it was much more comfortable after being washed several times and lasted years with heavy use.If you buy this jacket, wash it first thing to loosen it up a little bit.I strongly recommend Carhart clothing, but there is compromise with comfort for long wear and warmth.My recommendation is this jacket below:http://www.amazon.com/Carhartt-Mens-Duck-Active-Thermal-Lined/dp/B002WLOK1U/ref=sr_1_16?ie=UTF8&s;=apparel&qlEnable;=1&qid;=1284079028&sr;=1-16,I gave this to my fiance as a birthday present, i love Cahartt and i have other products from them.', 'What more can I say, except to fill up words required on this review.']\n",
      "\n",
      "-----Imperative:\n",
      "1127\n",
      "['Fits great and easy to  clean!', \"Can't complain about these shoes at all except that I'm worried about getting them dirty!,Love itWas a pr&auml;sentIt was verry NiceEvery BodyLiked itAnd gave onlyCompliment's on the item,I had been wearing converse for years I had them in every single color but never an all white pair lol so I ordered them and they just look amazing with anything u wear I usually wear a 5.5 in boys I ordered a 4.5 in men's they fit a little big so for next time I'll ordered a size 4.0 overall I live them shipping only took 2 days I would recommend.,I got these to replace a pair of the exact color that I got in 1995.\", \"Liked 'em more when I was 8.,One of the very few places that sell these above a size 13 and a few dollars cheaper than buying directly from Converse.These are the real deal, in case there were any concerns!,I bought these for my daughter who wore her previous Converse into the ground.\", \"Can't go wrong with the classics!,Great shoes!\", 'Arrange your boobs so they are comfortable and facing the same direction, then easiely hook the rest of the clasps from the bottom up.Ta-Da!I will say this is a LOT of bra.', \"Won't plan on wearing again but it's good enough for the one time I needed them.,Sure it fits snug - but that's what it is supposed to do!\", 'Thank you Amazon for making searching for items easy and having a better price!,I am a solid 32DDD.', \"Get a pair and get ready for lots of compliments when your baby wears them!,I bought these for my grandson who isn't here yet but can't to see them on him.\", \"See how the work in time but for my early impression of them, they seem very good!,It's a good t shirt.\", 'Meaning its not a rip off !', 'Was very pleased with my purchase!,This is a really cute watch and works very well.', 'Looks more like I have a tank top on than a bra!', 'Would buy again for sure!,The more I wear it, the more I love it.', \"Don't be afraid!\", 'Holds up without slippage, very sexy!', \"Save your money and buy them at the store to make sure it's a real K Swiss shoe because this shoe is terrible!!!!\", 'Be careful of size differences !,By far the most comfortable sandal I have ever owned.', 'Made well and fits perfect!', 'will definately order from this vendor again.,Very nice, very shiny!', 'Makes my shopping experience a whole lot easier!,I really like the fit and style of these pants.', 'Thank you ariat for making great looking boots!', 'Are very confortable and good looking..!', 'Thank a bunch BEARPAW rrrrrrrrrrrrrrrrrrrrrr.,very warm wife liked it,Great boot comfortable and good quality although short but the quality is beautiful I love bearpaws, you uggs people can keep spending your $$$$ on uggs I love bearpaw and I love amazon!!!!!!!!!', 'Looks great on him, made very good, warm and comes in tall sizes!!!', 'Buy a size up and adjust them to where they are comfortable!', 'Will be excellent to wear on my upcoming Philippines trip!', 'Adjusting the width wasnerve-racking because I had to be very careful not to break it!4.', 'Makes biking more comfortable and protects important parts of the anatomy!,I bought these for my son.', 'Fits very true to size and have been very happy with both bras so far!,While I had to return mine due to the sizing running rather small, I did like how soft the material felt and how comfortable the cups were around my breasts.', \"Keeps your warm and is thin and soft!,I've been real happy with this turtleneck, it fits perfect, the color is awesome, it launders beautifully and the fabric is soft and silky.\"]\n"
     ]
    }
   ],
   "source": [
    "# Extract Imperative & Question\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pyparsing import OneOrMore, nestedExpr\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "question_sentence = []\n",
    "imperative_sentence = []\n",
    "\n",
    "def analyze_sentence(text):  # using StanfordCoreNLP\n",
    "    output = nlp.annotate(text, properties={\n",
    "    'annotators': 'parse',\n",
    "    'outputFormat': 'json'\n",
    "    })\n",
    "    \n",
    "    if \"ADJP\" in output['sentences'][0]['parse']: # Adj Phrase Sentences\n",
    "        data = OneOrMore(nestedExpr()).parseString(output['sentences'][0]['parse'])  \n",
    "        \n",
    "        if ('?' in text or \"SBARQ\" in data[0][1][0] or \"SQ\" in data[0][1][0]) and ('!' not in text): # Question\n",
    "            question_sentence.append(text)\n",
    "        elif ('!' in text and '?' not in text) and (\"S\" == data[0][1][0] and \"VP\" == data[0][1][1][0]): # Imperative\n",
    "            imperative_sentence.append(text)\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "start = time.time()\n",
    "\n",
    "index = 0\n",
    "while index < len(textsplit):\n",
    "    try:\n",
    "        analyze_sentence(textsplit[index])\n",
    "    except:\n",
    "        continue\n",
    "    finally:\n",
    "        index+=1\n",
    "        \n",
    "        \n",
    "print('time:\\t ' , time.time()-start)\n",
    "print(\"-----Question:\")\n",
    "print(len(question_sentence))\n",
    "print(question_sentence[:30])\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_sentence))\n",
    "print(imperative_sentence[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.41437444543034\n"
     ]
    }
   ],
   "source": [
    "# Avg length of imperative sentence\n",
    "imp_len = 0\n",
    "for i in imperative_sentence:\n",
    "    imp_len += len(i)\n",
    "\n",
    "imp_avg_len = imp_len / len(imperative_sentence)\n",
    "\n",
    "print(imp_avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.81966322650734\n"
     ]
    }
   ],
   "source": [
    "# Avg length of question sentence\n",
    "ques_len = 0\n",
    "for i in question_sentence:\n",
    "    ques_len += len(i)\n",
    "\n",
    "ques_avg_len = ques_len / len(question_sentence)\n",
    "\n",
    "print(ques_avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.422053231939163\n"
     ]
    }
   ],
   "source": [
    "ques_token_len = 0\n",
    "ques_tokens = [nltk.word_tokenize(t) for t in question_sentence]\n",
    "\n",
    "for i in ques_tokens:\n",
    "    ques_token_len += len(i)\n",
    "\n",
    "ques_avg_token = ques_token_len / len(ques_tokens)\n",
    "\n",
    "print(ques_avg_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.35226264418811\n"
     ]
    }
   ],
   "source": [
    "imp_token_len = 0\n",
    "imp_tokens = [nltk.word_tokenize(t) for t in imperative_sentence]\n",
    "\n",
    "for i in imp_tokens:\n",
    "    imp_token_len += len(i)\n",
    "\n",
    "imp_avg_token = imp_token_len / len(imp_tokens)\n",
    "\n",
    "print(imp_avg_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------------+---------------------+----------------------+\n",
      "|         Type        | Numbers | Avg Length (Characters) | Avg Length (Tokens) | Percentage in Total  |\n",
      "+---------------------+---------+-------------------------+---------------------+----------------------+\n",
      "|  Question Sentence  |   3682  |    136.81966322650734   |  30.422053231939163 | 0.4056960519008877%  |\n",
      "| Imperative Sentence |   1127  |    90.41437444543034    |  20.35226264418811  | 0.12417692843354165% |\n",
      "+---------------------+---------+-------------------------+---------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# Create table to show data\n",
    "from prettytable import PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Type\", \"Numbers\", \"Avg Length (Characters)\", \"Avg Length (Tokens)\", \"Percentage in Total\"]\n",
    "table.add_row([\"Question Sentence\", len(question_sentence), ques_avg_len, ques_avg_token, str(len(question_sentence)/len(textsplit)*100) + \"%\"])\n",
    "table.add_row([\"Imperative Sentence\", len(imperative_sentence), imp_avg_len, imp_avg_token, str(len(imperative_sentence)/len(textsplit)*100) + \"%\"])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "112014\n",
      "['I', 'would', 'definitely', 'try', 'other', 'languages', 'after', 'seeing', 'just', 'how', 'easy', 'it', 'is', 'to', 'use', 'this', 'stuff.What', 'did', \"n't\", 'I']\n",
      "\n",
      "-----Imperative:\n",
      "22937\n",
      "['Fits', 'great', 'and', 'easy', 'to', 'clean', '!', 'Ca', \"n't\", 'complain', 'about', 'these', 'shoes', 'at', 'all', 'except', 'that', 'I', \"'m\", 'worried']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "# ==== Question ====    \n",
    "question_tokens = []\n",
    "for t in question_sentence:\n",
    "    question_tokens += nltk.word_tokenize(t) \n",
    "    \n",
    "# ==== Imperative ====    \n",
    "imperative_tokens = []\n",
    "for t in imperative_sentence:\n",
    "    imperative_tokens += nltk.word_tokenize(t) \n",
    "    \n",
    "print(\"-----Question:\")\n",
    "print(len(question_tokens))\n",
    "print(question_tokens[:20])\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_tokens))\n",
    "print(imperative_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "112014\n",
      "['i', 'would', 'definitely', 'try', 'other', 'languages', 'after', 'seeing', 'just', 'how', 'easy', 'it', 'is', 'to', 'use', 'this', 'stuff.what', 'did', \"n't\", 'i']\n",
      "\n",
      "-----Imperative:\n",
      "22937\n",
      "['fits', 'great', 'and', 'easy', 'to', 'clean', '!', 'ca', \"n't\", 'complain', 'about', 'these', 'shoes', 'at', 'all', 'except', 'that', 'i', \"'m\", 'worried']\n"
     ]
    }
   ],
   "source": [
    "# Lowercase \n",
    "\n",
    "# ==== Question ====\n",
    "question_lowercase = [w.lower() for w in question_tokens] \n",
    "\n",
    "# ==== Imperative ====\n",
    "imperative_lowercase = [w.lower() for w in imperative_tokens] \n",
    "\n",
    "print(\"-----Question:\")\n",
    "print(len(question_lowercase))\n",
    "print(question_lowercase[:20])\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_lowercase))\n",
    "print(imperative_lowercase[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "91179\n",
      "['i', 'would', 'definitely', 'try', 'other', 'languages', 'after', 'seeing', 'just', 'how', 'easy', 'it', 'is', 'to', 'use', 'this', 'did', 'i', 'like', 'for']\n",
      "\n",
      "-----Imperative:\n",
      "18609\n",
      "['fits', 'great', 'and', 'easy', 'to', 'clean', 'ca', 'complain', 'about', 'these', 'shoes', 'at', 'all', 'except', 'that', 'i', 'worried', 'about', 'getting', 'them']\n"
     ]
    }
   ],
   "source": [
    "# alphabets \n",
    "\n",
    "# ==== Question ====\n",
    "question_alphaWords = [w for w in question_lowercase if w.isalpha()]\n",
    "\n",
    "# ==== Imperative ====\n",
    "imperative_alphaWords = [w for w in imperative_lowercase if w.isalpha()]\n",
    "\n",
    "\n",
    "print(\"-----Question:\")\n",
    "print(len(question_alphaWords))\n",
    "print(question_alphaWords[:20])\n",
    "\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_alphaWords))\n",
    "print(imperative_alphaWords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:\t  20.58369517326355\n",
      "-----Question:\n",
      "91179\n",
      "['i', 'would', 'definitely', 'try', 'other', 'language', 'after', 'see', 'just', 'how', 'easy', 'it', 'be', 'to', 'use', 'this', 'do', 'i', 'like', 'for']\n",
      "\n",
      "-----Imperative:\n",
      "18609\n",
      "['fit', 'great', 'and', 'easy', 'to', 'clean', 'ca', 'complain', 'about', 'these', 'shoe', 'at', 'all', 'except', 'that', 'i', 'worried', 'about', 'get', 'them']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ==== Question ====\n",
    "question_lemmatizedWords = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in question_alphaWords]\n",
    "\n",
    "# ==== Imperative ====\n",
    "imperative_lemmatizedWords = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in imperative_alphaWords]\n",
    "\n",
    "\n",
    "print('time:\\t ' , time.time()-start)\n",
    "\n",
    "print(\"-----Question:\")\n",
    "print(len(question_lemmatizedWords))\n",
    "print(question_lemmatizedWords[:20])\n",
    "\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_lemmatizedWords))\n",
    "print(imperative_lemmatizedWords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "42504\n",
      "['would', 'definitely', 'try', 'language', 'see', 'easy', 'use', 'like', 'starter', 'headphone', 'microphone', 'come', 'quality', 'comfortable', 'ear', 'get', 'really', 'hot', 'slightly', 'disappointed']\n",
      "\n",
      "-----Imperative:\n",
      "9565\n",
      "['fit', 'great', 'easy', 'clean', 'ca', 'complain', 'shoe', 'except', 'worried', 'get', 'dirty', 'love', 'itwas', 'pr', 'auml', 'sentit', 'verry', 'niceevery', 'bodyliked', 'itand']\n"
     ]
    }
   ],
   "source": [
    "# Stop word list \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# ==== Question ====\n",
    "question_filtered_words = [w for w in question_lemmatizedWords if not w in stop_words ]\n",
    "\n",
    "# ==== Imperative ====\n",
    "imperative_filtered_words = [w for w in imperative_lemmatizedWords if not w in stop_words ]\n",
    "\n",
    "print(\"-----Question:\")\n",
    "print(len(question_filtered_words))\n",
    "print(question_filtered_words[:20])\n",
    "\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_filtered_words))\n",
    "print(imperative_filtered_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "5069\n",
      "['would', 'definitely', 'try', 'language', 'see', 'easy', 'use', 'like', 'starter', 'headphone', 'microphone', 'come', 'quality', 'comfortable', 'ear', 'get', 'really', 'hot', 'slightly', 'disappointed']\n",
      "\n",
      "-----Imperative:\n",
      "1891\n",
      "['fit', 'great', 'easy', 'clean', 'ca', 'complain', 'shoe', 'except', 'worried', 'get', 'dirty', 'love', 'itwas', 'pr', 'auml', 'sentit', 'verry', 'niceevery', 'bodyliked', 'itand']\n"
     ]
    }
   ],
   "source": [
    "# frequency distribution with FreqDist\n",
    "from nltk import FreqDist\n",
    "\n",
    "# ==== Question ====\n",
    "question_fdist = FreqDist(question_filtered_words)\n",
    "question_fdistkeys = list(question_fdist.keys())\n",
    "\n",
    "# ==== Imperative ====\n",
    "imperative_fdist = FreqDist(imperative_filtered_words)\n",
    "imperative_fdistkeys = list(imperative_fdist.keys())\n",
    "\n",
    "print(\"-----Question:\")\n",
    "print(len(question_fdistkeys))\n",
    "print(question_fdistkeys[:20])\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "print(len(imperative_fdistkeys))\n",
    "print(imperative_fdistkeys[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Unigram ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "('look', 528)\n",
      "('like', 522)\n",
      "('size', 481)\n",
      "('would', 463)\n",
      "('get', 443)\n",
      "('wear', 415)\n",
      "('fit', 414)\n",
      "('say', 401)\n",
      "('make', 377)\n",
      "('shoe', 376)\n",
      "('one', 348)\n",
      "('good', 343)\n",
      "('well', 334)\n",
      "('great', 323)\n",
      "('could', 300)\n",
      "('love', 273)\n",
      "('really', 258)\n",
      "('comfortable', 248)\n",
      "('want', 248)\n",
      "('watch', 242)\n",
      "('go', 237)\n",
      "('color', 235)\n",
      "('know', 227)\n",
      "('order', 226)\n",
      "('small', 224)\n",
      "('much', 219)\n",
      "('little', 218)\n",
      "('price', 215)\n",
      "('pair', 209)\n",
      "('right', 205)\n",
      "\n",
      "-----Imperative:\n",
      "('look', 329)\n",
      "('great', 260)\n",
      "('fit', 189)\n",
      "('get', 139)\n",
      "('love', 118)\n",
      "('size', 118)\n",
      "('comfortable', 116)\n",
      "('like', 115)\n",
      "('wear', 112)\n",
      "('make', 109)\n",
      "('good', 107)\n",
      "('go', 99)\n",
      "('nice', 94)\n",
      "('would', 88)\n",
      "('well', 85)\n",
      "('shoe', 79)\n",
      "('order', 77)\n",
      "('cute', 75)\n",
      "('buy', 62)\n",
      "('perfect', 62)\n",
      "('ca', 59)\n",
      "('price', 59)\n",
      "('sure', 58)\n",
      "('really', 57)\n",
      "('recommend', 55)\n",
      "('color', 53)\n",
      "('bought', 53)\n",
      "('small', 53)\n",
      "('shirt', 51)\n",
      "('pair', 50)\n"
     ]
    }
   ],
   "source": [
    "# 30 words by frequency\n",
    "\n",
    "# ==== Question ====\n",
    "print(\"-----Question:\")\n",
    "question_topkeys = question_fdist.most_common(30)\n",
    "for pair in question_topkeys:\n",
    "    print (pair)\n",
    "    \n",
    "# ==== Imperative ====\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "imperative_topkeys = imperative_fdist.most_common(30)\n",
    "for pair in imperative_topkeys:\n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def alpha_filter(w):\n",
    "    pattern = re.compile('^[a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "((',', 'i'), 0.00562429696287964)\n",
      "((',', 'but'), 0.0044994375703037125)\n",
      "(('?', 'i'), 0.004204831538914778)\n",
      "(('?', '?'), 0.0034638527326941275)\n",
      "(('?', ','), 0.003392433088721053)\n",
      "((',', 'and'), 0.0031781741568018282)\n",
      "(('&', '#'), 0.0031067545128287536)\n",
      "(('#', '34'), 0.0029014230364061634)\n",
      "(('34', ';'), 0.0029014230364061634)\n",
      "(('it', \"'s\"), 0.0025443248165407896)\n",
      "(('in', 'the'), 0.002508614994554252)\n",
      "(('they', 'are'), 0.0023925580730980054)\n",
      "(('of', 'the'), 0.0022586462406484903)\n",
      "(('?', ')'), 0.0022140089631653187)\n",
      "(('what', 'more'), 0.0021872265966754157)\n",
      "(('do', \"n't\"), 0.001964040209259557)\n",
      "(('i', \"'m\"), 0.0019194029317763852)\n",
      "(('it', 'is'), 0.0018390558323066759)\n",
      "(('?', 'the'), 0.0018301283768100416)\n",
      "(('?', 'what'), 0.0018122734658167728)\n",
      "\n",
      "-----Imperative:\n",
      "(('!', ','), 0.01813663513101103)\n",
      "(('!', '!'), 0.007760387147403758)\n",
      "((',', 'i'), 0.007673191786196974)\n",
      "(('!', 'looks'), 0.004577756463356149)\n",
      "((',', 'and'), 0.0037929982124950954)\n",
      "(('great', '!'), 0.0037058028512883113)\n",
      "(('looks', 'great'), 0.0034006190870645684)\n",
      "((',', 'this'), 0.0032698260452543927)\n",
      "((',', 'but'), 0.0032262283646510006)\n",
      "(('ca', \"n't\"), 0.002572263155600122)\n",
      "(('they', 'are'), 0.002572263155600122)\n",
      "((',', 'very'), 0.0024414701137899465)\n",
      "(('in', 'the'), 0.0023978724331865544)\n",
      "((',', 'the'), 0.0023542747525831624)\n",
      "(('for', 'my'), 0.002179884030169595)\n",
      "(('&', '#'), 0.0020490909883594194)\n",
      "(('#', '34'), 0.0020054933077560274)\n",
      "((',', 'these'), 0.0020054933077560274)\n",
      "(('34', ';'), 0.0020054933077560274)\n",
      "(('i', 'love'), 0.0018747002659458517)\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# ==== Question ====\n",
    "print(\"-----Question:\")\n",
    "question_finder = BigramCollocationFinder.from_words(question_lowercase)\n",
    "question_scored = question_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in question_scored[:20]:\n",
    "    print(bscore)\n",
    "    \n",
    "# ==== Imperative ====\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "imperative_finder = BigramCollocationFinder.from_words(imperative_lowercase)\n",
    "imperative_scored = imperative_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in imperative_scored[:20]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "(('in', 'the'), 0.002508614994554252)\n",
      "(('they', 'are'), 0.0023925580730980054)\n",
      "(('of', 'the'), 0.0022586462406484903)\n",
      "(('what', 'more'), 0.0021872265966754157)\n",
      "(('it', 'is'), 0.0018390558323066759)\n",
      "(('i', 'have'), 0.0015980145338975486)\n",
      "(('to', 'be'), 0.0015890870784009141)\n",
      "(('a', 'little'), 0.0014998125234345708)\n",
      "(('and', 'i'), 0.0014730301569446677)\n",
      "(('for', 'a'), 0.001455175245951399)\n",
      "(('is', 'a'), 0.0013748281464816899)\n",
      "(('for', 'the'), 0.0013569732354884211)\n",
      "(('and', 'the'), 0.0013480457799917868)\n",
      "(('i', 'am'), 0.0013480457799917868)\n",
      "(('on', 'the'), 0.0013480457799917868)\n",
      "(('can', 'you'), 0.0013123359580052493)\n",
      "(('i', 'say'), 0.0013123359580052493)\n",
      "(('can', 'i'), 0.0012676986805220775)\n",
      "(('if', 'you'), 0.0012319888585355402)\n",
      "(('i', 'was'), 0.0011427143035691967)\n",
      "\n",
      "-----Imperative:\n",
      "(('looks', 'great'), 0.0034006190870645684)\n",
      "(('they', 'are'), 0.002572263155600122)\n",
      "(('in', 'the'), 0.0023978724331865544)\n",
      "(('for', 'my'), 0.002179884030169595)\n",
      "(('i', 'love'), 0.0018747002659458517)\n",
      "(('it', 'is'), 0.0018747002659458517)\n",
      "(('a', 'little'), 0.001743907224135676)\n",
      "(('is', 'a'), 0.0017003095435322842)\n",
      "(('and', 'i'), 0.0016131141823255003)\n",
      "(('great', 'with'), 0.0015695165017221085)\n",
      "(('i', 'have'), 0.0015695165017221085)\n",
      "(('on', 'the'), 0.0015259188211187164)\n",
      "(('this', 'is'), 0.0014823211405153246)\n",
      "(('of', 'the'), 0.0014387234599119326)\n",
      "(('to', 'wear'), 0.0014387234599119326)\n",
      "(('to', 'be'), 0.0013951257793085408)\n",
      "(('comfortable', 'and'), 0.001351528098705149)\n",
      "(('for', 'a'), 0.001351528098705149)\n",
      "(('i', 'was'), 0.001351528098705149)\n",
      "(('fits', 'great'), 0.001307930418101757)\n"
     ]
    }
   ],
   "source": [
    "# alpha bigrams\n",
    "\n",
    "# ==== Question ====\n",
    "print(\"-----Question:\")\n",
    "question_finder.apply_word_filter(alpha_filter)\n",
    "question_scored1 = question_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in question_scored1[:20]:\n",
    "    print(bscore)\n",
    "    \n",
    "# ==== Imperative ====\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "imperative_finder.apply_word_filter(alpha_filter)\n",
    "imperative_scored1 = imperative_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in imperative_scored1[:20]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Bigram ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Question:\n",
      "(('go', 'wrong'), 0.00031246094238220225)\n",
      "(('well', 'made'), 0.00028567857589229916)\n",
      "(('year', 'old'), 0.00024104129840912743)\n",
      "(('look', 'like'), 0.00023211384291249308)\n",
      "(('good', 'quality'), 0.0001964040209259557)\n",
      "(('looks', 'great'), 0.0001964040209259557)\n",
      "(('would', 'recommend'), 0.00018747656542932134)\n",
      "(('feel', 'like'), 0.000178549109932687)\n",
      "(('another', 'pair'), 0.00016962165443605264)\n",
      "(('high', 'quality'), 0.0001606941989394183)\n",
      "(('make', 'sure'), 0.0001606941989394183)\n",
      "(('highly', 'recommend'), 0.00015176674344278393)\n",
      "(('fits', 'great'), 0.00014283928794614958)\n",
      "(('little', 'bit'), 0.00014283928794614958)\n",
      "(('looks', 'like'), 0.00014283928794614958)\n",
      "(('much', 'better'), 0.00014283928794614958)\n",
      "(('size', 'smaller'), 0.00013391183244951523)\n",
      "(('years', 'ago'), 0.00013391183244951523)\n",
      "(('look', 'good'), 0.00012498437695288088)\n",
      "(('one', 'size'), 0.00012498437695288088)\n",
      "\n",
      "-----Imperative:\n",
      "(('looks', 'great'), 0.0034006190870645684)\n",
      "(('fits', 'great'), 0.001307930418101757)\n",
      "(('go', 'wrong'), 0.001264332737498365)\n",
      "(('make', 'sure'), 0.0010463443344814055)\n",
      "(('looks', 'good'), 0.0009155512926712299)\n",
      "(('look', 'great'), 0.0006975628896542704)\n",
      "(('would', 'recommend'), 0.0006103675284474866)\n",
      "(('fits', 'perfect'), 0.000435976806033919)\n",
      "(('well', 'made'), 0.000435976806033919)\n",
      "(('would', 'buy'), 0.000435976806033919)\n",
      "(('fit', 'great'), 0.0003923791254305271)\n",
      "(('good', 'quality'), 0.0003923791254305271)\n",
      "(('highly', 'recommend'), 0.0003923791254305271)\n",
      "(('super', 'cute'), 0.0003923791254305271)\n",
      "(('feels', 'great'), 0.0003487814448271352)\n",
      "(('great', 'price'), 0.0003487814448271352)\n",
      "(('say', 'enough'), 0.0003487814448271352)\n",
      "(('fits', 'well'), 0.0003051837642237433)\n",
      "(('look', 'good'), 0.0003051837642237433)\n",
      "(('look', 'like'), 0.0003051837642237433)\n"
     ]
    }
   ],
   "source": [
    "# stopword bigrams  -> Bigrams Result\n",
    "\n",
    "# ==== Question ====\n",
    "print(\"-----Question:\")\n",
    "question_finder.apply_word_filter(lambda w: w in stop_words)\n",
    "question_scored2 = question_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in question_scored2[:20]:\n",
    "    print(bscore)\n",
    "    \n",
    "# ==== Imperative ====\n",
    "print()\n",
    "print(\"-----Imperative:\")\n",
    "imperative_finder.apply_word_filter(lambda w: w in stop_words)\n",
    "imperative_scored2 = imperative_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in imperative_scored2[:20]:\n",
    "    print(bscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
